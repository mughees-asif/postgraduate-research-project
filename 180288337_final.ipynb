{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:blue'><center>Recommendation System using Graph Neural Networks</center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "<b style='color:DodgerBlue'><center><a href='https://www.linkedin.com/in/mugheesasif/'>Mughees Asif</a></center></b>\n",
    "<b style='color:DodgerBlue'><center><a href='https://www.sems.qmul.ac.uk/staff/a.nanjangud'>Dr Angadh Nanjangud</a></center></b>\n",
    "<i style='color:rgb(0, 122, 172)'><center><a href='http://www.eecs.qmul.ac.uk/'>School of Electronic Engineering and Computer Science</a></center></i>\n",
    "<i style='color:rgb(0, 122, 172)'><center><a href='https://www.qmul.ac.uk/'>Queen Mary, University of London</a></center></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/hero.jpg' alt=\"introduction\" width=\"500px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "In recent years, recommendation systems have been developed with graph neural networks (GNNs) to expedite the aggregation process of macro (e.g., topological structure) and micro (e.g., node information) operations to enrich information filtering capabilities. Additionally, using neural networks for recommendation tasks on graphlike data structures has been proven to elicit meaningful representations of user-item relationships. However, the representation learning process is not linear as social relations augmented with item interactions, both need to be considered for optimal results. This research project aims to address this issue by proposing a novel framework that is capable of using multiple preferences for outlining recommendations/ratings of specific items to users. The framework is also split into three variations that are pitted against published academic recommendation systems. The training and testing of the models is done by using two real-world datasets that contain user-item and user-user information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/rating.png\" alt=\"rating\" style=\"width: 500px;\"/>\n",
    "<h5 align=\"center\">Figure 1: Research framework displaying the two main components needed to make a rating prediction</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Warning:</b> <li>The written content of this notebook mirrors the content in the <a href='/'>official PDF evaluation</a>.</li> <li>Please view this notebook <a href='/'>here</a> to see the complete open-source version.</li>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents<a class=\"anchor\" id=\"contents\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "**1** &nbsp;&nbsp;**[Import dependencies](#dw-dep)**<br>\n",
    "\n",
    "**2** &nbsp;&nbsp;**[Download data](#data)**<br>\n",
    "\n",
    "**3** &nbsp;&nbsp;**[Data processing](#preprocess-data)**<br>\n",
    "\n",
    "**4** &nbsp;&nbsp;**[Graph Neural Network](#gnn)**<br>\n",
    "\n",
    "**5** &nbsp;&nbsp;**[Training](#training)**<br>\n",
    "\n",
    "**6** &nbsp;&nbsp;**[Testing](#testing)**<br>\n",
    "\n",
    "**7** &nbsp;&nbsp;**[Results](#results)**<br>\n",
    "<!-- &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.1&nbsp;&nbsp;*[Live](#live)*<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6.2&nbsp;&nbsp;*[Recorded](#recorded)*<br>\n",
    " -->\n",
    "**8** &nbsp;&nbsp;**[References](#references)**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To return to the contents, press the üîù icon located in the title of each chapter.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1&nbsp;&nbsp;Import dependencies <a class=\"anchor\" id=\"dw-dep\"></a> [üîù](#contents)\n",
    "\n",
    "This section imports the necessary libraries needed to develop the GNN recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2&nbsp;&nbsp;Download data <a class=\"anchor\" id=\"data\"></a> [üîù](#contents)\n",
    "\n",
    "This section downloads the Ciao and Epinions data sets from the official [GitHub](https://github.com/mughees-asif/) repository of this project. They can also be sourced from [here](https://www.cse.msu.edu/~tangjili/datasetcode/truststudy.htm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sF0PfJNVU9pJ",
    "outputId": "3ec5b40c-8b8e-47b0-f828-941465f7032d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-23 07:02:25--  https://github.com/mughees-asif/graph-rec/raw/master/data.zip\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/mughees-asif/graph-rec/master/data.zip [following]\n",
      "--2022-06-23 07:02:25--  https://raw.githubusercontent.com/mughees-asif/graph-rec/master/data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 25230096 (24M) [application/zip]\n",
      "Saving to: ‚Äòdata.zip‚Äô\n",
      "\n",
      "data.zip            100%[===================>]  24.06M  --.-KB/s    in 0.1s    \n",
      "\n",
      "2022-06-23 07:02:27 (192 MB/s) - ‚Äòdata.zip‚Äô saved [25230096/25230096]\n",
      "\n",
      "Archive:  /content/data.zip\n",
      "  inflating: ciao/rating.mat         \n",
      "  inflating: ciao/trustnetwork.mat   \n",
      "  inflating: dataset_ciao.pkl        \n",
      "  inflating: dataset_epinions.pkl    \n",
      "  inflating: epinions/rating.mat     \n",
      "  inflating: epinions/trustnetwork.mat  \n",
      "  inflating: list_ciao.pkl           \n",
      "  inflating: list_epinions.pkl       \n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mughees-asif/graph-rec/raw/master/data.zip\n",
    "!unzip /content/data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViLefaDdRjf8"
   },
   "source": [
    "## 3&nbsp;&nbsp;Data processing <a class=\"anchor\" id=\"preprocess-data\"></a> [üîù](#contents)\n",
    "\n",
    "This section will highlight the loading of the data for neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "t8QImMumRjf9"
   },
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, data, u_items_list, u_user_list, u_users_items_list, i_users_list):\n",
    "        self.data = data\n",
    "        self.u_items_list = u_items_list\n",
    "        self.u_users_list = u_user_list\n",
    "        self.u_users_items_list = u_users_items_list\n",
    "        self.i_users_list = i_users_list\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        uid = self.data[index][0]\n",
    "        iid = self.data[index][1]\n",
    "        label = self.data[index][2]\n",
    "        u_items = self.u_items_list[uid]\n",
    "        u_users = self.u_users_list[uid]\n",
    "        u_users_items = self.u_users_items_list[uid]\n",
    "        i_users = self.i_users_list[iid]\n",
    "\n",
    "        return (uid, iid, label), u_items, u_users, u_users_items, i_users\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "YuzfO_naRjf9"
   },
   "outputs": [],
   "source": [
    "truncate_len = 45\n",
    "\n",
    "def collate_fn(batch_data):\n",
    "\n",
    "    uids, iids, labels = [], [], []\n",
    "    u_items, u_users, u_users_items, i_users = [], [], [], []\n",
    "    u_items_len, u_users_len, i_users_len = [], [], []\n",
    "\n",
    "    for data, u_items_u, u_users_u, u_users_items_u, i_users_i in batch_data:\n",
    "\n",
    "        (uid, iid, label) = data\n",
    "        uids.append(uid)\n",
    "        iids.append(iid)\n",
    "        labels.append(label)\n",
    "\n",
    "        # user-items\n",
    "        if len(u_items_u) <= truncate_len:\n",
    "            u_items.append(u_items_u)\n",
    "        else:\n",
    "            u_items.append(random.sample(u_items_u, truncate_len))\n",
    "        u_items_len.append(min(len(u_items_u), truncate_len))\n",
    "        \n",
    "        # user-users and user-users-items\n",
    "        if len(u_users_u) <= truncate_len:\n",
    "            u_users.append(u_users_u)\n",
    "            u_u_items = [] \n",
    "            for uui in u_users_items_u:\n",
    "                if len(uui) < truncate_len:\n",
    "                    u_u_items.append(uui)\n",
    "                else:\n",
    "                    u_u_items.append(random.sample(uui, truncate_len))\n",
    "            u_users_items.append(u_u_items)\n",
    "        else:\n",
    "            sample_index = random.sample(list(range(len(u_users_u))), truncate_len)\n",
    "            u_users.append([u_users_u[si] for si in sample_index])\n",
    "\n",
    "            u_users_items_u_tr = [u_users_items_u[si] for si in sample_index]\n",
    "            u_u_items = [] \n",
    "            for uui in u_users_items_u_tr:\n",
    "                if len(uui) < truncate_len:\n",
    "                    u_u_items.append(uui)\n",
    "                else:\n",
    "                    u_u_items.append(random.sample(uui, truncate_len))\n",
    "            u_users_items.append(u_u_items)\n",
    "\n",
    "        u_users_len.append(min(len(u_users_u), truncate_len))\t\n",
    "\n",
    "        # item-users\n",
    "        if len(i_users_i) <= truncate_len:\n",
    "            i_users.append(i_users_i)\n",
    "        else:\n",
    "            i_users.append(random.sample(i_users_i, truncate_len))\n",
    "        i_users_len.append(min(len(i_users_i), truncate_len))\n",
    "\n",
    "    batch_size = len(batch_data)\n",
    "\n",
    "    # padding\n",
    "    u_items_maxlen = max(u_items_len)\n",
    "    u_users_maxlen = max(u_users_len)\n",
    "    i_users_maxlen = max(i_users_len)\n",
    "    \n",
    "    u_item_pad = torch.zeros([batch_size, u_items_maxlen, 2], dtype=torch.long)\n",
    "    for i, ui in enumerate(u_items):\n",
    "        u_item_pad[i, :len(ui), :] = torch.LongTensor(ui)\n",
    "    \n",
    "    u_user_pad = torch.zeros([batch_size, u_users_maxlen], dtype=torch.long)\n",
    "    for i, uu in enumerate(u_users):\n",
    "        u_user_pad[i, :len(uu)] = torch.LongTensor(uu)\n",
    "    \n",
    "    u_user_item_pad = torch.zeros([batch_size, u_users_maxlen, u_items_maxlen, 2], dtype=torch.long)\n",
    "    for i, uu_items in enumerate(u_users_items):\n",
    "        for j, ui in enumerate(uu_items):\n",
    "            u_user_item_pad[i, j, :len(ui), :] = torch.LongTensor(ui)\n",
    "\n",
    "    i_user_pad = torch.zeros([batch_size, i_users_maxlen, 2], dtype=torch.long)\n",
    "    for i, iu in enumerate(i_users):\n",
    "        i_user_pad[i, :len(iu), :] = torch.LongTensor(iu)\n",
    "\n",
    "    uids = torch.LongTensor(uids)\n",
    "    iids = torch.LongTensor(iids)\n",
    "    labels = torch.FloatTensor(labels)\n",
    "\n",
    "    return uids, iids, labels, u_item_pad, u_user_pad, u_user_item_pad, i_user_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7FHQ7iyRjf_"
   },
   "source": [
    "## 4&nbsp;&nbsp;Graph Neural Network (GNN)<a class=\"anchor\" id=\"gnn\"></a> [üîù](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "0CNFKW9vRjf_"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, input_dim//2, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim//2, output_dim, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class Aggregator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Aggregator, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim, bias=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class UserModel(nn.Module):\n",
    "    def __init__(self, emb_dim, user_emb, item_emb, rating_emb):\n",
    "        super(UserModel, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.user_emb = user_emb\n",
    "        self.item_emb = item_emb\n",
    "        self.rating_emb = rating_emb\n",
    "\n",
    "        self.g_v = MLP(2*self.emb_dim, self.emb_dim)\n",
    "        \n",
    "        self.user_item_attn = MLP(2*self.emb_dim, 1)\n",
    "        self.aggr_items = Aggregator(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.user_user_attn = MLP(2*self.emb_dim, 1)\n",
    "        self.aggr_neighbors = Aggregator(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim, self.emb_dim, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim, bias = True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim, bias = True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.eps = 1e-10\n",
    "\n",
    "    def forward(self, uids, u_item_pad, u_user_pad, u_user_item_pad):\n",
    "\n",
    "        q_a = self.item_emb(u_item_pad[:,:,0])\n",
    "        u_item_er = self.rating_emb(u_item_pad[:,:,1])\n",
    "        x_ia = self.g_v(torch.cat([q_a, u_item_er], dim=2).view(-1, 2*self.emb_dim)).view(q_a.size())\n",
    "        mask_u = torch.where(u_item_pad[:,:,0]>0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        p_i = mask_u.unsqueeze(2).expand_as(x_ia) * self.user_emb(uids).unsqueeze(1).expand_as(x_ia)\n",
    "        alpha = self.user_item_attn(torch.cat([x_ia, p_i], dim=2).view(-1, 2*self.emb_dim)).view(mask_u.size())\n",
    "        alpha = torch.exp(alpha)*mask_u\n",
    "        alpha = alpha / (torch.sum(alpha, 1).unsqueeze(1).expand_as(alpha) + self.eps)\n",
    "        h_iI = self.aggr_items(torch.sum(alpha.unsqueeze(2).expand_as(x_ia) * x_ia, 1))\n",
    "\n",
    "\n",
    "        q_a_s = self.item_emb(u_user_item_pad[:,:,:,0])\n",
    "        u_user_item_er = self.rating_emb(u_user_item_pad[:,:,:,1])\n",
    "        x_ia_s = self.g_v(torch.cat([q_a_s, u_user_item_er], dim=2).view(-1, 2*self.emb_dim)).view(q_a_s.size())\n",
    "        mask_s = torch.where(u_user_item_pad[:,:,:,0]>0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        p_i_s = mask_s.unsqueeze(3).expand_as(x_ia_s) * self.user_emb(u_user_pad).unsqueeze(2).expand_as(x_ia_s)\n",
    "        alpha_s = self.user_item_attn(torch.cat([x_ia_s, p_i_s], dim=3).view(-1, 2*self.emb_dim)).view(mask_s.size())\n",
    "        alpha_s = torch.exp(alpha_s)*mask_s\n",
    "        alpha_s = alpha_s / (torch.sum(alpha_s, 2).unsqueeze(2).expand_as(alpha_s) + self.eps)\n",
    "        h_oI_temp = torch.sum(alpha_s.unsqueeze(3).expand_as(x_ia_s) * x_ia_s, 2)\n",
    "        h_oI = self.aggr_items(h_oI_temp.view(-1, self.emb_dim)).view(h_oI_temp.size())\n",
    "        \n",
    "        beta = self.user_user_attn(torch.cat([h_oI, self.user_emb(u_user_pad)], dim = 2).view(-1, 2 * self.emb_dim)).view(u_user_pad.size())\n",
    "        mask_su = torch.where(u_user_pad > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        beta = torch.exp(beta) * mask_su\n",
    "        beta = beta / (torch.sum(beta, 1).unsqueeze(1).expand_as(beta) + self.eps)\n",
    "        h_iS = self.aggr_neighbors(torch.sum(beta.unsqueeze(2).expand_as(h_oI) * h_oI, 1))\n",
    "\n",
    "        h_i = self.mlp(torch.cat([h_iI, h_iS], dim = 1))\n",
    "\n",
    "        return h_i\n",
    "\n",
    "\n",
    "class ItemModel(nn.Module):\n",
    "    def __init__(self, emb_dim, user_emb, item_emb, rating_emb):\n",
    "        super(ItemModel, self).__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.user_emb = user_emb\n",
    "        self.item_emb = item_emb\n",
    "        self.rating_emb = rating_emb\n",
    "\n",
    "        self.g_u = MLP(2*self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.item_users_attn = MLP(2*self.emb_dim, 1)\n",
    "        self.aggr_users = Aggregator(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.eps = 1e-10\n",
    "    \n",
    "    def forward(self, iids, i_user_pad):\n",
    "\n",
    "        p_t = self.user_emb(i_user_pad[:,:,0])\n",
    "        i_user_er = self.rating_emb(i_user_pad[:,:,1])\n",
    "        mask_i = torch.where(i_user_pad[:,:,0] > 0, torch.tensor([1.], device=self.device), torch.tensor([0.], device=self.device))\n",
    "        f_jt = self.g_u(torch.cat([p_t, i_user_er], dim = 2).view(-1, 2 * self.emb_dim)).view(p_t.size())\n",
    "        q_j = mask_i.unsqueeze(2).expand_as(f_jt) * self.item_emb(iids).unsqueeze(1).expand_as(f_jt)\n",
    "        mu_jt = self.item_users_attn(torch.cat([f_jt, q_j], dim = 2).view(-1, 2 * self.emb_dim)).view(mask_i.size())\n",
    "        mu_jt = torch.exp(mu_jt) * mask_i\n",
    "        mu_jt = mu_jt / (torch.sum(mu_jt, 1).unsqueeze(1).expand_as(mu_jt) + self.eps)\n",
    "        \n",
    "        z_j = self.aggr_users(torch.sum(mu_jt.unsqueeze(2).expand_as(f_jt) * f_jt, 1))\n",
    "\n",
    "        return z_j\n",
    "        \n",
    "    \n",
    "class GraphRec(nn.Module):\n",
    "    def __init__(self, n_users, n_items, n_ratings, emb_dim = 64):\n",
    "        super(GraphRec, self).__init__()\n",
    "        self.n_users = n_users\n",
    "        self.n_items = n_items\n",
    "        self.n_ratings = n_ratings\n",
    "        self.emb_dim = emb_dim\n",
    "\n",
    "        self.user_emb = nn.Embedding(self.n_users, self.emb_dim, padding_idx=0)\n",
    "        self.item_emb = nn.Embedding(self.n_items, self.emb_dim, padding_idx=0)\n",
    "        self.rating_emb = nn.Embedding(self.n_ratings, self.emb_dim, padding_idx=0)\n",
    "\n",
    "        self.user_model = UserModel(self.emb_dim, self.user_emb, self.item_emb, self.rating_emb)\n",
    "        self.item_model = ItemModel(self.emb_dim, self.user_emb, self.item_emb, self.rating_emb)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*self.emb_dim, self.emb_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, self.emb_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.emb_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, uids, iids, u_item_pad, u_user_pad, u_user_item_pad, i_user_pad):\n",
    "\n",
    "        h_i = self.user_model(uids, u_item_pad, u_user_pad, u_user_item_pad)\n",
    "        z_j = self.item_model(iids, i_user_pad)\n",
    "\n",
    "        r_ij = self.mlp(torch.cat([h_i, z_j], dim=1))\n",
    "\n",
    "        return r_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQarCNCvRjgA"
   },
   "source": [
    "## 5&nbsp;&nbsp;Training <a class=\"anchor\" id=\"training\"></a> [üîù](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-y6jE-UnRjgB",
    "outputId": "43ea0c8f-5d5d-4193-feec-b1a1d91ac6cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device - cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device - ' + str(device))\n",
    "batch_size = 128\n",
    "embed_dim = 64\n",
    "learning_rate = 0.001\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfNuENP0RjgC"
   },
   "source": [
    "### Read dataset and preprocess it to form batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "2o_KviG9RjgC"
   },
   "outputs": [],
   "source": [
    "with open('/content/dataset_epinions.pkl', 'rb') as f:\n",
    "    train_set = pickle.load(f)\n",
    "    valid_set = pickle.load(f)\n",
    "    test_set = pickle.load(f)\n",
    "\n",
    "with open('/content/list_epinions.pkl', 'rb') as f:\n",
    "    u_items_list = pickle.load(f)\n",
    "    u_users_list = pickle.load(f)\n",
    "    u_users_items_list = pickle.load(f)\n",
    "    i_users_list = pickle.load(f)\n",
    "    (user_count, item_count, rate_count) = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ChoqM7cnRjgD"
   },
   "outputs": [],
   "source": [
    "train_data = GraphDataset(train_set, u_items_list, u_users_list, u_users_items_list, i_users_list)\n",
    "valid_data = GraphDataset(valid_set, u_items_list, u_users_list, u_users_items_list, i_users_list)\n",
    "test_data = GraphDataset(test_set, u_items_list, u_users_list, u_users_items_list, i_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "PkkTqKF7RjgE"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True, collate_fn = collate_fn)\n",
    "valid_loader = DataLoader(valid_data, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False, collate_fn = collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vxMptIhpRjgE",
    "outputId": "c44f6986-39c9-40f0-c290-e375394cfeb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5704"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p7973lj2RjgF"
   },
   "source": [
    "### Create the model and set up training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "UreJlQt7RjgF"
   },
   "outputs": [],
   "source": [
    "model = GraphRec(user_count+1, item_count+1, rate_count+1, embed_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "RIUxFlKzRjgF"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(model.parameters(), learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 4, gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aqhnTKi1RjgF"
   },
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaj75cFoRjgF",
    "outputId": "2be4ae6b-3981-4ae5-f36b-b1a77f4e0b8f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:38<00:00, 10.99it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 validation: MAE: 1.2495, RMSE: 3.4318, Best MAE: 1.2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:31<00:00, 11.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 validation: MAE: 1.2535, RMSE: 3.4167, Best MAE: 1.2495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:31<00:00, 11.15it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 validation: MAE: 1.2467, RMSE: 3.4076, Best MAE: 1.2467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:29<00:00, 11.19it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 validation: MAE: 1.2377, RMSE: 3.4060, Best MAE: 1.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:30<00:00, 11.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 validation: MAE: 1.2378, RMSE: 3.4035, Best MAE: 1.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:30<00:00, 11.17it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 validation: MAE: 1.2368, RMSE: 3.4032, Best MAE: 1.2368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:31<00:00, 11.16it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 validation: MAE: 1.2357, RMSE: 3.4003, Best MAE: 1.2357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:30<00:00, 11.18it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 validation: MAE: 1.2354, RMSE: 3.3996, Best MAE: 1.2354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:30<00:00, 11.17it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 validation: MAE: 1.2349, RMSE: 3.4000, Best MAE: 1.2349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5704/5704 [08:30<00:00, 11.16it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:42<00:00, 16.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 validation: MAE: 1.2350, RMSE: 3.4004, Best MAE: 1.2349\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # Training step\n",
    "    model.train()\n",
    "    s_loss = 0\n",
    "    for i, (uids, iids, labels, u_items, u_users, u_users_items, i_users) in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        uids = uids.to(device)\n",
    "        iids = iids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        u_items = u_items.to(device)\n",
    "        u_users = u_users.to(device)\n",
    "        u_users_items = u_users_items.to(device)\n",
    "        i_users = i_users.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_val = loss.item()\n",
    "        s_loss += loss_val\n",
    "        \n",
    "        iter_num = epoch * len(train_loader) + i + 1\n",
    "\n",
    "    # Validate step\n",
    "    model.eval()\n",
    "    errors = []\n",
    "    with torch.no_grad():\n",
    "        for uids, iids, labels, u_items, u_users, u_users_items, i_users in tqdm(valid_loader):\n",
    "            uids = uids.to(device)\n",
    "            iids = iids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            u_items = u_items.to(device)\n",
    "            u_users = u_users.to(device)\n",
    "            u_users_items = u_users_items.to(device)\n",
    "            i_users = i_users.to(device)\n",
    "            preds = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
    "            error = torch.abs(preds.squeeze(1) - labels)\n",
    "            errors.extend(error.data.cpu().numpy().tolist())\n",
    "    \n",
    "    mae = np.mean(errors)\n",
    "    rmse = np.sqrt(np.mean(np.power(errors, 2)))\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    ckpt_dict = {\n",
    "        'epoch': epoch + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(ckpt_dict, 'trained_models/latest_checkpoint.pth')\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_mae = mae\n",
    "    elif mae < best_mae:\n",
    "        best_mae = mae\n",
    "        torch.save(ckpt_dict, 'trained_models/best_checkpoint_{}.pth'.format(embed_dim))\n",
    "\n",
    "    print('Epoch {} validation: MAE: {:.4f}, RMSE: {:.4f}, Best MAE: {:.4f}'.format(epoch+1, mae, rmse, best_mae))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFD-FzMERjgG"
   },
   "source": [
    "## 6&nbsp;&nbsp;Testing <a class=\"anchor\" id=\"testing\"></a> [üîù](#contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "22savj_zRjgG",
    "outputId": "0855d26c-488b-4d7a-fd5b-da6fef5b4e4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 64\n",
    "checkpoint = torch.load('trained_models/best_checkpoint_{}.pth'.format(embed_dim))\n",
    "model = GraphRec(user_count+1, item_count+1, rate_count+1, embed_dim).to(device)\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj8XYjY4RjgG",
    "outputId": "d008b0c1-636f-4a0b-d9d2-334a85162bb5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 713/713 [00:43<00:00, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: MAE: 1.2399, RMSE: 3.4053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_errors = []\n",
    "with torch.no_grad():\n",
    "    for uids, iids, labels, u_items, u_users, u_users_items, i_users in tqdm(test_loader):\n",
    "        uids = uids.to(device)\n",
    "        iids = iids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        u_items = u_items.to(device)\n",
    "        u_users = u_users.to(device)\n",
    "        u_users_items = u_users_items.to(device)\n",
    "        i_users = i_users.to(device)\n",
    "        preds = model(uids, iids, u_items, u_users, u_users_items, i_users)\n",
    "        error = torch.abs(preds.squeeze(1) - labels)\n",
    "        test_errors.extend(error.data.cpu().numpy().tolist())\n",
    "\n",
    "test_mae = np.mean(test_errors)\n",
    "test_rmse = np.sqrt(np.mean(np.power(test_errors, 2)))\n",
    "print('Test: MAE: {:.4f}, RMSE: {:.4f}'.format(test_mae, test_rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8c9Pxa8xRjgH"
   },
   "source": [
    "## 7&nbsp;&nbsp;Results <a class=\"anchor\" id=\"results\"></a> [üîù](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8&nbsp;&nbsp;References <a class=\"anchor\" id=\"references\"></a> [üîù](#contents)\n",
    "\n",
    "<sup>1</sup>Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. ‚ÄúGenerative Adversarial Networks: An Overview‚Äù. In: *IEEE Signal Processing Magazine* 35.1 (2018), pp. 53‚Äì65.<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To cite, please use the following information:**\n",
    "\n",
    "_BibTeX_:\n",
    "\n",
    "`@misc{asif_nanjangud_2022,`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`title = {Recommendation System using Graph Neural Networks},`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`url = {https://github.com/mughees-asif/__________},`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`journal = {GitHub},`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`publisher = {Mughees Asif},`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`author = {Asif, Mughees and Nanjangud, Angadh},`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`year = {2022},`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ‚ÄÉ`month = {Aug}`<br />\n",
    "‚ÄÉ‚ÄÉ‚ÄÉ`}`\n",
    "\n",
    "_Harvard_:\n",
    "\n",
    "`Asif, M., and Nanjangud, A.. (2022). Recommendation System using Graph Neural Networks.`\n",
    "\n",
    "_APA_:\n",
    "\n",
    "`Asif, M., & Nanjangud, A.. (2022). Recommendation System using Graph Neural Networks.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='color:purple'><center><a href='#contents'>üîù</a>&#8592;&#8592;&#8592;&#8592; END &#8594;&#8594;&#8594;&#8594;<a href='#contents'>üîù</a></center></h1>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "main.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "interpreter": {
   "hash": "ec7d137c6e0bfe5aec1849c5c512dd0bf44cf2eb2fae9fc2de49724729b3d6c6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
